{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "cv = cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from albumentations import HorizontalFlip, Normalize, Compose\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "\n",
    "from catalyst.dl import SupervisedRunner, MetricCallback\n",
    "\n",
    "seed = 69\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLE-Mask utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4)'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, mean, std, phase, catalyst=False):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, mean, std)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "        self.catalyst = catalyst\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] # 1x256x1600x4\n",
    "        mask = mask[0].permute(2, 0, 1) # 1x4x256x1600\n",
    "        \n",
    "        if self.catalyst:\n",
    "            return {'targets': mask, 'features': img}\n",
    "        else:\n",
    "            return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "\n",
    "def get_transforms(phase, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "                HorizontalFlip(), # only horizontal flip as of now\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    catalyst=False,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    # some preprocessing\n",
    "    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "    df['ClassId'] = df['ClassId'].astype(int)\n",
    "    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "    df['defects'] = df.count(axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"])\n",
    "    \n",
    "    train_dataset = SteelDataset(train_df, data_folder, mean, std, 'train', catalyst=catalyst)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "    \n",
    "    val_dataset = SteelDataset(val_df, data_folder, mean, std, 'val', catalyst=catalyst)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = './dataset/sample_submission.csv'\n",
    "train_df_path = './dataset/train.csv'\n",
    "data_folder = \"./dataset/\"\n",
    "test_data_folder = \"./dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import metrics\n",
    "from catalyst.utils import get_activation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCallback(MetricCallback):\n",
    "    \"\"\"\n",
    "    Dice metric callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_key: str = \"targets\",\n",
    "        output_key: str = \"logits\",\n",
    "        prefix: str = \"dice\",\n",
    "        eps: float = 1e-7,\n",
    "        threshold: float = None,\n",
    "        activation: str = \"Sigmoid\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param input_key: input key to use for dice calculation;\n",
    "            specifies our `y_true`.\n",
    "        :param output_key: output key to use for dice calculation;\n",
    "            specifies our `y_pred`.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            prefix=prefix,\n",
    "            metric_fn=self.dice,\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            eps=eps,\n",
    "            threshold=threshold,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "    def dice(self, outputs, targets, eps=1e-7, threshold=0.5, activation='Sigmoid'):\n",
    "        activation_fn = get_activation_fn(activation)\n",
    "        outputs = activation_fn(outputs)\n",
    "\n",
    "        if threshold is not None:\n",
    "            outputs = (outputs > threshold).float()\n",
    "            \n",
    "        batch_size = len(targets)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        targets = targets.view(batch_size, -1)\n",
    "\n",
    "        intersection = torch.sum(targets * outputs, dim=1)\n",
    "        union = torch.sum(targets, dim=1) + torch.sum(outputs, dim=1)\n",
    "        dice = (2 * intersection / (union + eps)).cpu().numpy()\n",
    "        \n",
    "        result = []\n",
    "        for i, d in enumerate(dice):\n",
    "            if d >= eps:\n",
    "                result.append(d)\n",
    "                continue\n",
    "            \n",
    "            s = torch.sum(targets[i]).cpu().numpy()\n",
    "            result.append(1 if s < eps else d)\n",
    "\n",
    "        return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IouCallback(MetricCallback):\n",
    "    \"\"\"\n",
    "    IoU (Jaccard) metric callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_key: str = \"targets\",\n",
    "        output_key: str = \"logits\",\n",
    "        prefix: str = \"iou\",\n",
    "        eps: float = 1e-7,\n",
    "        threshold: float = None,\n",
    "        activation: str = \"Sigmoid\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_key (str): input key to use for iou calculation\n",
    "                specifies our ``y_true``.\n",
    "            output_key (str): output key to use for iou calculation;\n",
    "                specifies our ``y_pred``\n",
    "            prefix (str): key to store in logs\n",
    "            eps (float): epsilon to avoid zero division\n",
    "            threshold (float): threshold for outputs binarization\n",
    "            activation (str): An torch.nn activation applied to the outputs.\n",
    "                Must be one of ['none', 'Sigmoid', 'Softmax2d']\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            prefix=prefix,\n",
    "            metric_fn=self.iou,\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            eps=eps,\n",
    "            threshold=threshold,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "    def iou(self, outputs: torch.Tensor,\n",
    "            targets: torch.Tensor,\n",
    "            eps: float = 1e-7,\n",
    "            threshold: float = None,\n",
    "            activation: str = \"Sigmoid\"\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (torch.Tensor): A list of predicted elements\n",
    "            targets (torch.Tensor):  A list of elements that are to be predicted\n",
    "            eps (float): epsilon to avoid zero division\n",
    "            threshold (float): threshold for outputs binarization\n",
    "            activation (str): An torch.nn activation applied to the outputs.\n",
    "                Must be one of [\"none\", \"Sigmoid\", \"Softmax2d\"]\n",
    "\n",
    "        Returns:\n",
    "            float: IoU (Jaccard) score\n",
    "        \"\"\"\n",
    "        activation_fn = get_activation_fn(activation)\n",
    "        outputs = activation_fn(outputs)\n",
    "\n",
    "        if threshold is not None:\n",
    "            outputs = (outputs > threshold).float()\n",
    "            \n",
    "        batch_size = len(targets)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        targets = targets.view(batch_size, -1)\n",
    "\n",
    "        intersection = torch.sum(targets * outputs, dim=1)\n",
    "        union = torch.sum(targets, dim=1) + torch.sum(outputs, dim=1)\n",
    "        iou = (intersection / (union - intersection + eps)).cpu().numpy()\n",
    "        \n",
    "        result = []\n",
    "        for i, d in enumerate(iou):\n",
    "            if d >= eps:\n",
    "                result.append(d)\n",
    "                continue\n",
    "            \n",
    "            s = torch.sum(targets[i]).cpu().numpy()\n",
    "            result.append(1 if s < eps else d)\n",
    "\n",
    "        return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet50\", encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./logdir/1.1.resnet50_1e4_adam_weightdecay\"\n",
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "default_batch_size = 32\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader =  provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=batch_size,\n",
    "                num_workers=6,\n",
    "    catalyst=True\n",
    "            )\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "# optimizer = SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import OptimizerCallback, CriterionCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "0/50 * Epoch (train): 100% 1257/1257 [09:42<00:00,  2.16it/s, _timers/_fps=609.870, dice=1.000, iou=1.000, loss=0.050]\n",
      "0/50 * Epoch (valid): 100% 315/315 [00:41<00:00,  7.50it/s, _timers/_fps=877.171, dice=0.245, iou=0.162, loss=0.066]\n",
      "[2019-09-02 15:51:05,794] \n",
      "0/50 * Epoch 0 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=537.4291 | _timers/batch_time=0.0161 | _timers/data_time=0.0012 | _timers/model_time=0.0149 | dice=0.6277 | iou=0.5885 | loss=0.0999\n",
      "0/50 * Epoch 0 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=599.4297 | _timers/batch_time=0.0186 | _timers/data_time=0.0035 | _timers/model_time=0.0151 | dice=0.7305 | iou=0.6730 | loss=0.0520\n",
      "1/50 * Epoch (train): 100% 1257/1257 [09:42<00:00,  2.16it/s, _timers/_fps=600.237, dice=0.770, iou=0.741, loss=0.026]\n",
      "1/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.48it/s, _timers/_fps=860.039, dice=0.372, iou=0.296, loss=0.039]\n",
      "[2019-09-02 16:01:36,260] \n",
      "1/50 * Epoch 1 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=519.6294 | _timers/batch_time=0.0164 | _timers/data_time=0.0010 | _timers/model_time=0.0153 | dice=0.7009 | iou=0.6489 | loss=0.0398\n",
      "1/50 * Epoch 1 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=635.6409 | _timers/batch_time=0.0164 | _timers/data_time=0.0030 | _timers/model_time=0.0134 | dice=0.6450 | iou=0.6038 | loss=0.0292\n",
      "2/50 * Epoch (train): 100% 1257/1257 [09:45<00:00,  2.15it/s, _timers/_fps=604.410, dice=0.815, iou=0.742, loss=0.010]\n",
      "2/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.44it/s, _timers/_fps=891.173, dice=0.811, iou=0.726, loss=0.012]\n",
      "[2019-09-02 16:12:09,602] \n",
      "2/50 * Epoch 2 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=512.1798 | _timers/batch_time=0.0166 | _timers/data_time=0.0010 | _timers/model_time=0.0156 | dice=0.7255 | iou=0.6703 | loss=0.0239\n",
      "2/50 * Epoch 2 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=623.2663 | _timers/batch_time=0.0171 | _timers/data_time=0.0035 | _timers/model_time=0.0135 | dice=0.7193 | iou=0.6642 | loss=0.0190\n",
      "3/50 * Epoch (train): 100% 1257/1257 [09:43<00:00,  2.15it/s, _timers/_fps=545.964, dice=0.639, iou=0.541, loss=0.014]\n",
      "3/50 * Epoch (valid): 100% 315/315 [00:41<00:00,  7.52it/s, _timers/_fps=884.688, dice=0.939, iou=0.891, loss=0.009]\n",
      "[2019-09-02 16:22:40,119] \n",
      "3/50 * Epoch 3 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=530.9837 | _timers/batch_time=0.0160 | _timers/data_time=0.0011 | _timers/model_time=0.0149 | dice=0.7408 | iou=0.6853 | loss=0.0176\n",
      "3/50 * Epoch 3 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=631.0464 | _timers/batch_time=0.0174 | _timers/data_time=0.0031 | _timers/model_time=0.0142 | dice=0.7425 | iou=0.6882 | loss=0.0155\n",
      "4/50 * Epoch (train): 100% 1257/1257 [09:44<00:00,  2.15it/s, _timers/_fps=602.262, dice=0.827, iou=0.772, loss=0.011]\n",
      "4/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.49it/s, _timers/_fps=853.607, dice=0.554, iou=0.528, loss=0.015]\n",
      "[2019-09-02 16:33:11,611] \n",
      "4/50 * Epoch 4 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=523.7988 | _timers/batch_time=0.0162 | _timers/data_time=0.0010 | _timers/model_time=0.0152 | dice=0.7514 | iou=0.6952 | loss=0.0144\n",
      "4/50 * Epoch 4 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=627.5648 | _timers/batch_time=0.0171 | _timers/data_time=0.0033 | _timers/model_time=0.0138 | dice=0.7495 | iou=0.6906 | loss=0.0138\n",
      "5/50 * Epoch (train): 100% 1257/1257 [09:44<00:00,  2.15it/s, _timers/_fps=583.626, dice=0.842, iou=0.775, loss=0.007]\n",
      "5/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.46it/s, _timers/_fps=885.388, dice=0.605, iou=0.559, loss=0.003]   \n",
      "[2019-09-02 16:43:43,827] \n",
      "5/50 * Epoch 5 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=520.2592 | _timers/batch_time=0.0164 | _timers/data_time=0.0010 | _timers/model_time=0.0153 | dice=0.7697 | iou=0.7098 | loss=0.0130\n",
      "5/50 * Epoch 5 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=617.4074 | _timers/batch_time=0.0179 | _timers/data_time=0.0036 | _timers/model_time=0.0142 | dice=0.7660 | iou=0.7050 | loss=0.0125\n",
      "6/50 * Epoch (train): 100% 1257/1257 [09:47<00:00,  2.14it/s, _timers/_fps=572.152, dice=0.785, iou=0.665, loss=0.012]\n",
      "6/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.44it/s, _timers/_fps=855.369, dice=0.813, iou=0.685, loss=0.004]    \n",
      "[2019-09-02 16:54:19,075] \n",
      "6/50 * Epoch 6 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=530.4015 | _timers/batch_time=0.0160 | _timers/data_time=0.0010 | _timers/model_time=0.0149 | dice=0.7845 | iou=0.7231 | loss=0.0116\n",
      "6/50 * Epoch 6 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=643.3842 | _timers/batch_time=0.0175 | _timers/data_time=0.0040 | _timers/model_time=0.0134 | dice=0.7934 | iou=0.7308 | loss=0.0116\n",
      "7/50 * Epoch (train):  99% 1245/1257 [09:43<00:05,  2.13it/s, _timers/_fps=567.670, dice=0.948, iou=0.915, loss=0.002]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "7/50 * Epoch (train): 100% 1257/1257 [09:49<00:00,  2.13it/s, _timers/_fps=563.874, dice=0.765, iou=0.724, loss=0.025]\n",
      "7/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.47it/s, _timers/_fps=891.765, dice=0.464, iou=0.304, loss=0.021]    \n",
      "[2019-09-02 17:04:52,747] \n",
      "7/50 * Epoch 7 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=530.2379 | _timers/batch_time=0.0161 | _timers/data_time=0.0011 | _timers/model_time=0.0149 | dice=0.7943 | iou=0.7319 | loss=0.0106\n",
      "7/50 * Epoch 7 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=642.5806 | _timers/batch_time=0.0163 | _timers/data_time=0.0034 | _timers/model_time=0.0128 | dice=0.7368 | iou=0.6757 | loss=0.0136\n",
      "8/50 * Epoch (train): 100% 1257/1257 [09:44<00:00,  2.15it/s, _timers/_fps=615.158, dice=0.872, iou=0.805, loss=0.006]    \n",
      "8/50 * Epoch (valid): 100% 315/315 [00:41<00:00,  7.53it/s, _timers/_fps=856.658, dice=0.607, iou=0.560, loss=0.003]    \n",
      "[2019-09-02 17:15:24,159] \n",
      "8/50 * Epoch 8 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=528.9844 | _timers/batch_time=0.0161 | _timers/data_time=0.0011 | _timers/model_time=0.0150 | dice=0.8014 | iou=0.7385 | loss=0.0101\n",
      "8/50 * Epoch 8 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=632.5627 | _timers/batch_time=0.0172 | _timers/data_time=0.0030 | _timers/model_time=0.0142 | dice=0.7904 | iou=0.7288 | loss=0.0107\n",
      "9/50 * Epoch (train):  95% 1192/1257 [09:17<00:30,  2.15it/s, _timers/_fps=520.224, dice=0.939, iou=0.903, loss=0.005]    Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "9/50 * Epoch (train): 100% 1257/1257 [09:47<00:00,  2.14it/s, _timers/_fps=549.955, dice=0.624, iou=0.520, loss=0.014]    \n",
      "9/50 * Epoch (valid): 100% 315/315 [00:42<00:00,  7.38it/s, _timers/_fps=790.781, dice=0.882, iou=0.789, loss=0.022]    \n",
      "[2019-09-02 17:25:57,166] \n",
      "9/50 * Epoch 9 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=499.8197 | _timers/batch_time=0.0171 | _timers/data_time=0.0010 | _timers/model_time=0.0161 | dice=0.8130 | iou=0.7492 | loss=0.0095\n",
      "9/50 * Epoch 9 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=628.7813 | _timers/batch_time=0.0171 | _timers/data_time=0.0042 | _timers/model_time=0.0129 | dice=0.7866 | iou=0.7227 | loss=0.0142\n",
      "10/50 * Epoch (train): 100% 1257/1257 [09:47<00:00,  2.14it/s, _timers/_fps=591.977, dice=0.935, iou=0.892, loss=0.012]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/50 * Epoch (valid): 100% 315/315 [00:41<00:00,  7.55it/s, _timers/_fps=868.588, dice=1.000, iou=1.000, loss=5.933e-04]\n",
      "[2019-09-02 17:36:28,360] \n",
      "10/50 * Epoch 10 (train): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=499.1667 | _timers/batch_time=0.0172 | _timers/data_time=0.0011 | _timers/model_time=0.0161 | dice=0.8178 | iou=0.7539 | loss=0.0090\n",
      "10/50 * Epoch 10 (valid): _base/lr=0.0001 | _base/momentum=0.9000 | _timers/_fps=632.7639 | _timers/batch_time=0.0176 | _timers/data_time=0.0032 | _timers/model_time=0.0144 | dice=0.8068 | iou=0.7409 | loss=0.0108\n",
      "11/50 * Epoch (train):  51% 646/1257 [05:00<04:40,  2.18it/s, _timers/_fps=411.090, dice=0.798, iou=0.718, loss=0.004]    "
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "#         CriterionCallback(),\n",
    "#         OptimizerCallback(accumulation_steps=32 // batch_size),\n",
    "        DiceCallback(threshold=0.5),\n",
    "               IouCallback(threshold=0.5),\n",
    "              ],\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

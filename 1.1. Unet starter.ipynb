{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "cv = cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from albumentations import HorizontalFlip, Normalize, Compose\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "\n",
    "from catalyst.dl import SupervisedRunner, MetricCallback\n",
    "\n",
    "seed = 69\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLE-Mask utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4)'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, mean, std, phase, catalyst=False):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, mean, std)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "        self.catalyst = catalyst\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] # 1x256x1600x4\n",
    "        mask = mask[0].permute(2, 0, 1) # 1x4x256x1600\n",
    "        \n",
    "        if self.catalyst:\n",
    "            return {'targets': mask, 'features': img}\n",
    "        else:\n",
    "            return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "\n",
    "def get_transforms(phase, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "                HorizontalFlip(), # only horizontal flip as of now\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    catalyst=False,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    # some preprocessing\n",
    "    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "    df['ClassId'] = df['ClassId'].astype(int)\n",
    "    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "    df['defects'] = df.count(axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"])\n",
    "    \n",
    "    train_dataset = SteelDataset(train_df, data_folder, mean, std, 'train', catalyst=catalyst)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "    \n",
    "    val_dataset = SteelDataset(val_df, data_folder, mean, std, 'val', catalyst=catalyst)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more utility functions\n",
    "\n",
    "Dice and IoU metric implementations, metric logger for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    '''X is sigmoid output of the model'''\n",
    "    X_p = torch.sigmoid(X)\n",
    "    preds = (X_p > threshold).type(torch.uint8)\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''   \n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "        \n",
    "        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.append(dice)\n",
    "        self.dice_pos_scores.append(dice_pos)\n",
    "        self.dice_neg_scores.append(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.mean(self.base_dice_scores)\n",
    "        dice_neg = np.mean(self.dice_neg_scores)\n",
    "        dice_pos = np.mean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou\n",
    "\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f\" % (epoch_loss, iou, dice, dice_neg, dice_pos))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and torch.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = (pred_c & label_c).sum()\n",
    "        union = (pred_c | label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append((intersection / union).cpu().numpy())\n",
    "    return ious if ious else [1]\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = outputs # copy is imp\n",
    "    labels = labels # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (layer1): DecoderBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): DecoderBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): DecoderBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): DecoderBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer5): DecoderBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 6, \"val\": 6}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4 * 1.5\n",
    "        self.num_epochs = 20\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        train_dataloader, val_dataloader = provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size['train'],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "        self.dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, (images, targets) in tqdm_notebook(enumerate(dataloader), total=len(dataloader)): # replace `dataloader` with `tk0` for tqdm\n",
    "            images = images.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            val_loss = self.iterate(epoch, \"val\")\n",
    "            self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model.pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = './dataset/sample_submission.csv'\n",
    "train_df_path = './dataset/train.csv'\n",
    "data_folder = \"./dataset/\"\n",
    "test_data_folder = \"./dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trainer = Trainer(model)\n",
    "# model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import metrics\n",
    "from catalyst.utils import get_activation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCallback(MetricCallback):\n",
    "    \"\"\"\n",
    "    Dice metric callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_key: str = \"targets\",\n",
    "        output_key: str = \"logits\",\n",
    "        prefix: str = \"dice\",\n",
    "        eps: float = 1e-7,\n",
    "        threshold: float = None,\n",
    "        activation: str = \"Sigmoid\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param input_key: input key to use for dice calculation;\n",
    "            specifies our `y_true`.\n",
    "        :param output_key: output key to use for dice calculation;\n",
    "            specifies our `y_pred`.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            prefix=prefix,\n",
    "            metric_fn=self.dice,\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            eps=eps,\n",
    "            threshold=threshold,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "    def dice(self, outputs, targets, eps=1e-7, threshold=0.5, activation='Sigmoid'):\n",
    "        activation_fn = get_activation_fn(activation)\n",
    "        outputs = activation_fn(outputs)\n",
    "\n",
    "        if threshold is not None:\n",
    "            outputs = (outputs > threshold).float()\n",
    "            \n",
    "        batch_size = len(targets)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        targets = targets.view(batch_size, -1)\n",
    "\n",
    "        intersection = torch.sum(targets * outputs, dim=1)\n",
    "        union = torch.sum(targets, dim=1) + torch.sum(outputs, dim=1)\n",
    "        dice = (2 * intersection / (union + eps)).cpu().numpy()\n",
    "        \n",
    "        result = []\n",
    "        for i, d in enumerate(dice):\n",
    "            if d >= eps:\n",
    "                result.append(d)\n",
    "                continue\n",
    "            \n",
    "            s = torch.sum(targets[i]).cpu().numpy()\n",
    "            result.append(1 if s < eps else d)\n",
    "\n",
    "        return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IouCallback(MetricCallback):\n",
    "    \"\"\"\n",
    "    IoU (Jaccard) metric callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_key: str = \"targets\",\n",
    "        output_key: str = \"logits\",\n",
    "        prefix: str = \"iou\",\n",
    "        eps: float = 1e-7,\n",
    "        threshold: float = None,\n",
    "        activation: str = \"Sigmoid\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_key (str): input key to use for iou calculation\n",
    "                specifies our ``y_true``.\n",
    "            output_key (str): output key to use for iou calculation;\n",
    "                specifies our ``y_pred``\n",
    "            prefix (str): key to store in logs\n",
    "            eps (float): epsilon to avoid zero division\n",
    "            threshold (float): threshold for outputs binarization\n",
    "            activation (str): An torch.nn activation applied to the outputs.\n",
    "                Must be one of ['none', 'Sigmoid', 'Softmax2d']\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            prefix=prefix,\n",
    "            metric_fn=self.iou,\n",
    "            input_key=input_key,\n",
    "            output_key=output_key,\n",
    "            eps=eps,\n",
    "            threshold=threshold,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "    def iou(self, outputs: torch.Tensor,\n",
    "            targets: torch.Tensor,\n",
    "            eps: float = 1e-7,\n",
    "            threshold: float = None,\n",
    "            activation: str = \"Sigmoid\"\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (torch.Tensor): A list of predicted elements\n",
    "            targets (torch.Tensor):  A list of elements that are to be predicted\n",
    "            eps (float): epsilon to avoid zero division\n",
    "            threshold (float): threshold for outputs binarization\n",
    "            activation (str): An torch.nn activation applied to the outputs.\n",
    "                Must be one of [\"none\", \"Sigmoid\", \"Softmax2d\"]\n",
    "\n",
    "        Returns:\n",
    "            float: IoU (Jaccard) score\n",
    "        \"\"\"\n",
    "        activation_fn = get_activation_fn(activation)\n",
    "        outputs = activation_fn(outputs)\n",
    "\n",
    "        if threshold is not None:\n",
    "            outputs = (outputs > threshold).float()\n",
    "            \n",
    "        batch_size = len(targets)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        targets = targets.view(batch_size, -1)\n",
    "\n",
    "        intersection = torch.sum(targets * outputs, dim=1)\n",
    "        union = torch.sum(targets, dim=1) + torch.sum(outputs, dim=1)\n",
    "        iou = (intersection / (union - intersection + eps)).cpu().numpy()\n",
    "        \n",
    "        result = []\n",
    "        for i, d in enumerate(iou):\n",
    "            if d >= eps:\n",
    "                result.append(d)\n",
    "                continue\n",
    "            \n",
    "            s = torch.sum(targets[i]).cpu().numpy()\n",
    "            result.append(1 if s < eps else d)\n",
    "\n",
    "        return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./logdir_fp32\"\n",
    "num_epochs = 50\n",
    "batch_size = 6\n",
    "default_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader =  provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=batch_size,\n",
    "                num_workers=6,\n",
    "    catalyst=True\n",
    "            )\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3 * batch_size / default_batch_size)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  1.29it/s, _timers/_fps=6.593, dice=0.003, iou=0.002, loss=0.024]  \n",
      "0/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.16it/s, _timers/_fps=1881.557, dice=0.622, iou=0.582, loss=0.013]\n",
      "[2019-08-30 10:38:53,002] \n",
      "0/50 * Epoch 0 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=193.2564 | _timers/batch_time=0.0325 | _timers/data_time=0.0008 | _timers/model_time=0.0317 | dice=0.5129 | iou=0.4994 | loss=0.0493\n",
      "0/50 * Epoch 0 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1380.5524 | _timers/batch_time=0.0062 | _timers/data_time=0.0019 | _timers/model_time=0.0043 | dice=0.5359 | iou=0.5148 | loss=0.0261\n",
      "1/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.21it/s, _timers/_fps=191.429, dice=0.665, iou=0.623, loss=0.053]\n",
      "1/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 12.44it/s, _timers/_fps=1841.492, dice=0.966, iou=0.944, loss=0.003]\n",
      "[2019-08-30 10:46:39,780] \n",
      "1/50 * Epoch 1 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.5627 | _timers/batch_time=0.0318 | _timers/data_time=0.0007 | _timers/model_time=0.0310 | dice=0.5998 | iou=0.5650 | loss=0.0256\n",
      "1/50 * Epoch 1 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1488.7158 | _timers/batch_time=0.0056 | _timers/data_time=0.0017 | _timers/model_time=0.0039 | dice=0.6014 | iou=0.5673 | loss=0.0225\n",
      "2/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=189.693, dice=0.664, iou=0.622, loss=0.105]\n",
      "2/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 13.22it/s, _timers/_fps=1976.891, dice=0.757, iou=0.688, loss=0.010]\n",
      "[2019-08-30 10:54:26,816] \n",
      "2/50 * Epoch 2 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.3988 | _timers/batch_time=0.0319 | _timers/data_time=0.0008 | _timers/model_time=0.0311 | dice=0.6233 | iou=0.5844 | loss=0.0234\n",
      "2/50 * Epoch 2 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1485.5189 | _timers/batch_time=0.0057 | _timers/data_time=0.0018 | _timers/model_time=0.0039 | dice=0.6335 | iou=0.5936 | loss=0.0194\n",
      "3/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=189.634, dice=1.000, iou=1.000, loss=0.013]\n",
      "3/50 * Epoch (valid): 100% 419/419 [00:38<00:00, 10.84it/s, _timers/_fps=1266.970, dice=0.704, iou=0.616, loss=0.027]\n",
      "[2019-08-30 11:02:19,148] \n",
      "3/50 * Epoch 3 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.4356 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.6448 | iou=0.6029 | loss=0.0213\n",
      "3/50 * Epoch 3 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=746.6488 | _timers/batch_time=0.0127 | _timers/data_time=0.0046 | _timers/model_time=0.0081 | dice=0.6961 | iou=0.6464 | loss=0.0181\n",
      "4/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=190.162, dice=0.750, iou=0.750, loss=0.005]\n",
      "4/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 12.28it/s, _timers/_fps=1503.245, dice=0.603, iou=0.566, loss=0.068]\n",
      "[2019-08-30 11:10:07,249] \n",
      "4/50 * Epoch 4 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=191.1076 | _timers/batch_time=0.0318 | _timers/data_time=0.0009 | _timers/model_time=0.0308 | dice=0.6578 | iou=0.6141 | loss=0.0194\n",
      "4/50 * Epoch 4 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1402.5309 | _timers/batch_time=0.0062 | _timers/data_time=0.0018 | _timers/model_time=0.0043 | dice=0.6729 | iou=0.6273 | loss=0.0171\n",
      "5/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=189.520, dice=0.682, iou=0.641, loss=0.009]\n",
      "5/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 12.44it/s, _timers/_fps=1848.933, dice=0.555, iou=0.469, loss=0.026]\n",
      "[2019-08-30 11:17:54,183] \n",
      "5/50 * Epoch 5 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.7563 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.6773 | iou=0.6302 | loss=0.0182\n",
      "5/50 * Epoch 5 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1490.6820 | _timers/batch_time=0.0056 | _timers/data_time=0.0017 | _timers/model_time=0.0039 | dice=0.6012 | iou=0.5649 | loss=0.0198\n",
      "6/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.20it/s, _timers/_fps=190.081, dice=0.399, iou=0.345, loss=0.061]\n",
      "6/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 13.22it/s, _timers/_fps=1985.626, dice=0.594, iou=0.565, loss=0.004]\n",
      "[2019-08-30 11:25:40,780] \n",
      "6/50 * Epoch 6 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.5237 | _timers/batch_time=0.0318 | _timers/data_time=0.0007 | _timers/model_time=0.0310 | dice=0.6925 | iou=0.6431 | loss=0.0173\n",
      "6/50 * Epoch 6 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1435.6903 | _timers/batch_time=0.0060 | _timers/data_time=0.0018 | _timers/model_time=0.0042 | dice=0.6687 | iou=0.6235 | loss=0.0196\n",
      "7/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.21it/s, _timers/_fps=193.952, dice=0.750, iou=0.750, loss=0.018]\n",
      "7/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 13.21it/s, _timers/_fps=1982.498, dice=0.633, iou=0.584, loss=0.009]\n",
      "[2019-08-30 11:33:27,448] \n",
      "7/50 * Epoch 7 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.7997 | _timers/batch_time=0.0317 | _timers/data_time=0.0007 | _timers/model_time=0.0310 | dice=0.7030 | iou=0.6524 | loss=0.0163\n",
      "7/50 * Epoch 7 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1454.6913 | _timers/batch_time=0.0059 | _timers/data_time=0.0018 | _timers/model_time=0.0040 | dice=0.6433 | iou=0.5991 | loss=0.0172\n",
      "8/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.21it/s, _timers/_fps=189.437, dice=0.815, iou=0.688, loss=0.022]\n",
      "8/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 13.14it/s, _timers/_fps=1145.672, dice=0.563, iou=0.511, loss=0.007]\n",
      "[2019-08-30 11:41:15,066] \n",
      "8/50 * Epoch 8 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.6839 | _timers/batch_time=0.0317 | _timers/data_time=0.0007 | _timers/model_time=0.0310 | dice=0.7050 | iou=0.6543 | loss=0.0158\n",
      "8/50 * Epoch 8 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1388.8996 | _timers/batch_time=0.0063 | _timers/data_time=0.0019 | _timers/model_time=0.0043 | dice=0.6834 | iou=0.6356 | loss=0.0151\n",
      "9/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.19it/s, _timers/_fps=190.293, dice=0.582, iou=0.477, loss=0.018]\n",
      "9/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.04it/s, _timers/_fps=2034.753, dice=0.787, iou=0.753, loss=0.007]\n",
      "[2019-08-30 11:49:03,048] \n",
      "9/50 * Epoch 9 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.6083 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7108 | iou=0.6601 | loss=0.0152\n",
      "9/50 * Epoch 9 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1247.9979 | _timers/batch_time=0.0076 | _timers/data_time=0.0019 | _timers/model_time=0.0057 | dice=0.7038 | iou=0.6536 | loss=0.0150\n",
      "10/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=190.183, dice=0.750, iou=0.750, loss=0.006]\n",
      "10/50 * Epoch (valid): 100% 419/419 [00:33<00:00, 12.39it/s, _timers/_fps=1755.674, dice=0.576, iou=0.521, loss=0.015]\n",
      "[2019-08-30 11:56:50,729] \n",
      "10/50 * Epoch 10 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.6348 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7219 | iou=0.6693 | loss=0.0147\n",
      "10/50 * Epoch 10 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1463.0798 | _timers/batch_time=0.0058 | _timers/data_time=0.0017 | _timers/model_time=0.0041 | dice=0.7304 | iou=0.6771 | loss=0.0144\n",
      "11/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.20it/s, _timers/_fps=189.793, dice=1.000, iou=1.000, loss=0.001]\n",
      "11/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.21it/s, _timers/_fps=1979.379, dice=0.708, iou=0.652, loss=0.008]\n",
      "[2019-08-30 12:04:37,754] \n",
      "11/50 * Epoch 11 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.5904 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7217 | iou=0.6701 | loss=0.0141\n",
      "11/50 * Epoch 11 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1412.2300 | _timers/batch_time=0.0064 | _timers/data_time=0.0019 | _timers/model_time=0.0045 | dice=0.6605 | iou=0.6128 | loss=0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.20it/s, _timers/_fps=188.659, dice=0.752, iou=0.644, loss=0.005]\n",
      "12/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.02it/s, _timers/_fps=1962.860, dice=0.793, iou=0.728, loss=0.020]\n",
      "[2019-08-30 12:12:25,453] \n",
      "12/50 * Epoch 12 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.6282 | _timers/batch_time=0.0319 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7278 | iou=0.6755 | loss=0.0137\n",
      "12/50 * Epoch 12 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1375.0067 | _timers/batch_time=0.0068 | _timers/data_time=0.0022 | _timers/model_time=0.0046 | dice=0.7132 | iou=0.6609 | loss=0.0143\n",
      "13/50 * Epoch (train): 100% 1676/1676 [07:11<00:00,  4.19it/s, _timers/_fps=189.962, dice=1.000, iou=1.000, loss=0.000]\n",
      "13/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.21it/s, _timers/_fps=1783.419, dice=0.708, iou=0.643, loss=0.017]\n",
      "[2019-08-30 12:20:12,625] \n",
      "13/50 * Epoch 13 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.7285 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7297 | iou=0.6775 | loss=0.0134\n",
      "13/50 * Epoch 13 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1268.3752 | _timers/batch_time=0.0072 | _timers/data_time=0.0020 | _timers/model_time=0.0051 | dice=0.6733 | iou=0.6266 | loss=0.0182\n",
      "14/50 * Epoch (train): 100% 1676/1676 [07:12<00:00,  4.21it/s, _timers/_fps=189.368, dice=0.934, iou=0.895, loss=0.011]\n",
      "14/50 * Epoch (valid): 100% 419/419 [00:34<00:00, 13.23it/s, _timers/_fps=1962.860, dice=0.807, iou=0.725, loss=0.010]\n",
      "[2019-08-30 12:28:00,370] \n",
      "14/50 * Epoch 14 (train): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=190.7741 | _timers/batch_time=0.0318 | _timers/data_time=0.0008 | _timers/model_time=0.0310 | dice=0.7351 | iou=0.6826 | loss=0.0129\n",
      "14/50 * Epoch 14 (valid): _base/lr=0.0015 | _base/momentum=0.9000 | _timers/_fps=1368.4005 | _timers/batch_time=0.0064 | _timers/data_time=0.0018 | _timers/model_time=0.0046 | dice=0.7492 | iou=0.6928 | loss=0.0140\n",
      "15/50 * Epoch (train):  47% 789/1676 [03:23<03:47,  3.89it/s, _timers/_fps=192.712, dice=0.847, iou=0.770, loss=0.012]"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    callbacks=[DiceCallback(threshold=0.5), IouCallback(threshold=0.5)],\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
